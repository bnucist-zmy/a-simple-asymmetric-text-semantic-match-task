{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习方法：BERT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集信息：\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 3 columns):\n",
      "label       8000 non-null int64\n",
      "question    8000 non-null object\n",
      "sentence    8000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 187.6+ KB\n",
      "————————————————\n",
      "数据内容概览：\n",
      "   label                                    question  \\\n",
      "0      0    When did the third Digimon series begin?   \n",
      "1      0    What famous palace is located in London?   \n",
      "2      0                 Who starred in 'True Love'?   \n",
      "3      0  What do most open education sources offer?   \n",
      "4      1                    How is Nirvana achieved?   \n",
      "\n",
      "                                            sentence  \n",
      "0  Unlike the two seasons before it and most of t...  \n",
      "1  London contains four World Heritage Sites: the...  \n",
      "2  The show starred Ted Danson as Dr. John Becker...  \n",
      "3  The conventional merit-system degree is curren...  \n",
      "4  In Theravada Buddhism, the ultimate goal is th...  \n",
      "得到样本：8000个\n"
     ]
    }
   ],
   "source": [
    "train_data=pd.read_csv('train.csv')\n",
    "test_data=pd.read_csv('test.csv')\n",
    "print(\"数据集信息：\")\n",
    "train_data.info()\n",
    "print(\"————————————————\")\n",
    "print(\"数据内容概览：\")\n",
    "print(train_data.head())\n",
    "print('得到样本：{}个'.format(train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] What was Beringia? [SEP] According to the prevailing theories of the settlement of the Americas, migrations of humans from Asia (in particular North Asia) to the Americas took place via Beringia, a land bridge which connected the two continents across what is now the Bering Strait. [SEP]\n",
      "[CLS] When did the third Digimon series begin? [SEP] Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese. [SEP]\n"
     ]
    }
   ],
   "source": [
    "#提取question和sentence并处理成 bert的输入形式，即添加CLS和SEP\n",
    "train_texts=[]\n",
    "test_texts=[]\n",
    "for i,row in train_data.iterrows():\n",
    "    one_text='[CLS] '+row['question']+' [SEP] '+row['sentence']+' [SEP]'\n",
    "    train_texts.append(one_text)\n",
    "for i,row in test_data.iterrows():\n",
    "    one_text='[CLS] '+row['question']+' [SEP] '+row['sentence']+' [SEP]'\n",
    "    test_texts.append(one_text)\n",
    "train_labels=list(train_data.label)\n",
    "\n",
    "print(test_texts[0])\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对文本进行tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized的第一句话: ['[CLS]', 'when', 'did', 'the', 'third', 'dig', '##imo', '##n', 'series', 'begin', '?', '[SEP]', 'unlike', 'the', 'two', 'seasons', 'before', 'it', 'and', 'most', 'of', 'the', 'seasons', 'that', 'followed', ',', 'dig', '##imo', '##n', 'tame', '##rs', 'takes', 'a', 'darker', 'and', 'more', 'realistic', 'approach', 'to', 'its', 'story', 'featuring', 'dig', '##imo', '##n', 'who', 'do', 'not', 'rein', '##car', '##nate', 'after', 'their', 'deaths', 'and', 'more', 'complex', 'character', 'development', 'in', 'the', 'original', 'japanese', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#tokenize 上面得到的text\n",
    "#这里要尽量用好的网络，否则会提示匹配不到model name，参考transformer 的github项目的issue\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "\n",
    "tokenized_train_texts=[tokenizer.tokenize(sent)[:512] for sent in train_texts]\n",
    "tokenized_test_texts=[tokenizer.tokenize(sent)[:512] for sent in test_texts]\n",
    "\n",
    "print(\"tokenized的第一句话:\",tokenized_train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将每一个token转换为词典里的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转化后的第一个句子: [101, 2043, 2106, 1996, 2353, 10667, 16339, 2078, 2186, 4088, 1029, 102, 4406, 1996, 2048, 3692, 2077, 2009, 1998, 2087, 1997, 1996, 3692, 2008, 2628, 1010, 10667, 16339, 2078, 24763, 2869, 3138, 1037, 9904, 1998, 2062, 12689, 3921, 2000, 2049, 2466, 3794, 10667, 16339, 2078, 2040, 2079, 2025, 27788, 10010, 12556, 2044, 2037, 6677, 1998, 2062, 3375, 2839, 2458, 1999, 1996, 2434, 2887, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "#将每个text中的token转换为  tokenizer vocabulary的id\n",
    "train_tokens_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_train_texts]\n",
    "test_tokens_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_test_texts]\n",
    "\n",
    "print(\"转化后的第一个句子:\",train_tokens_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACFCAYAAACg7bhYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAALZklEQVR4nO3dXYxd1XmH8eeP+UqbKg54QLbHromwKlDVQGQRR/SCQkoBoUAliIKq4EaufEMrIkVqaStREfWC3IQkaoSKCoqpogBtEoEsJKCGqOpFIDbfxEWYiOKxLWzER1pFSTG8vThr0rF97DnY58zx7PP8pK2997uXZ6/XOvOeNeus2ZOqQpLULSeNuwOSpOGzuEtSB1ncJamDLO6S1EEWd0nqIIu7JHXQvMU9yelJnkryXJKXktzW4uckeTLJK0nuT3Jqi5/Wzne262tGm4Ik6VCDjNx/BVxaVZ8ELgCuSLIe+BpwR1WtBd4GNrb2G4G3q+pc4I7WTpK0gOYt7tXzP+30lLYVcCnwry2+Gbi2HV/TzmnXL0uSofVYkjSvkwdplGQJsB04F/g28CrwTlUdaE1mgJXteCWwC6CqDiR5FzgTePNIX3/ZsmW1Zs2aY+m/JE2s7du3v1lVU/2uDVTcq+p94IIkS4EfAuf1a9b2/Ubphz3jIMkmYBPA6tWr2bZt2yBdkSQ1Sf7rSNc+1GqZqnoH+BGwHliaZPbNYRrY045ngFXtxicDHwPe6vO17qqqdVW1bmqq7xuPJOkYDbJaZqqN2EnyEeCzwA7gCeC61mwD8GA7fqid064/Xj6dTJIW1CDTMsuBzW3e/STggarakuSnwH1J/h54Bri7tb8b+OckO+mN2L8wgn5Lko5i3uJeVc8DF/aJ/wy4qE/8l8D1Q+ndAlgxvZq9u3cdFFu+chV7Zl4fU48k6fgN9IFql+3dvYvLv7rloNijt149pt5I0nD4+AFJ6iCLuyR1kMVdkjrI4i5JHWRxl6QOsrhLUgdZ3Ps4ackpJDloWzG9etzdkqSBTfw6934+eP89175LWtQcuUtSB1ncJamDLO6S1EEWd0nqIIu7JHWQxV2SOsjiLkkdZHGXpA6yuEtSB1ncJamDLO6S1EEW9wH5MDFJi4kPDhuQDxOTtJhMzMh9xfTqw0beScbdLUkaiYkZue/dveuwkTc4+pbUTfOO3JOsSvJEkh1JXkpyc4ufkeSxJK+0/cdbPEm+lWRnkueTfGrUSUiSDjbItMwB4CtVdR6wHrgpyfnALcDWqloLbG3nAFcCa9u2Cbhz6L2WJB3VvMW9qvZW1dPt+L+BHcBK4Bpgc2u2Gbi2HV8D3Fs9PwaWJlk+9J5Lko7oQ32gmmQNcCHwJHB2Ve2F3hsAcFZrthLYNeefzbTYoV9rU5JtSbbt37//w/dcknREAxf3JB8Fvg98uap+frSmfWJ1WKDqrqpaV1XrpqamBu2GJGkAAxX3JKfQK+zfraoftPAbs9Mtbb+vxWeAVXP++TSwZzjdlSQNYpDVMgHuBnZU1dfnXHoI2NCONwAPzonf2FbNrAfenZ2+kSQtjEHWuV8MfBF4IcmzLfY3wO3AA0k2Aq8D17drDwNXATuBXwBfGmqPJUnzmre4V9V/0H8eHeCyPu0LuOk4+yVJOg4T8/gBSZokFndJ6iCLuyR1kMVdkjrI4i5JHWRxl6QOsrhLUgdZ3CWpgyzuktRBFndJ6iCLuyR1kMVdkjrI4i5JHWRxl6QOsrhLUgdZ3CWpgyzuktRBFvfjcNKSU0hy0LZievW4uyVJA/0NVR3BB++/x+Vf3XJQ7NFbrx5TbyTp/zlyl6QOsrhLUgdZ3CWpgyzuktRBnSzuK6ZXH7aKRZImybyrZZLcA1wN7Kuq322xM4D7gTXAa8Dnq+rt9KroN4GrgF8Af1pVT4+m60e2d/eusa1imV0eOdfylavYM/P6gtxfkmCwpZDfAf4BuHdO7BZga1XdnuSWdv5XwJXA2rZ9Griz7SeGyyMlnQjmnZapqn8H3jokfA2wuR1vBq6dE7+3en4MLE2yfFidlSQN5ljn3M+uqr0AbX9Wi68Eds1pN9Nih0myKcm2JNv2799/jN2QJPUz7A9U+31yWf0aVtVdVbWuqtZNTU0NuRuSNNmOtbi/MTvd0vb7WnwGWDWn3TSw59i7J0k6Fsda3B8CNrTjDcCDc+I3pmc98O7s9I0kaeEMshTye8AlwLIkM8DfAbcDDyTZCLwOXN+aP0xvGeROekshvzSCPkuS5jFvca+qG45w6bI+bQu46Xg7JUk6Pp38DVVJmnQWd0nqIIu7JHWQxV2SOsjiLkkdZHGXpA6yuC+A2ccAz91WTK8ed7ckddggj/zVcfIxwJIWmiN3Seogi/uYOFUjaZSclhkTp2okjZIjd0nqIIu7JHWQxV2SOsjiLkkdZHE/gbiCRtKwuFrmBOIKGknDsuhH7iumVx822pWkSbfoR+57d+9ytCtJh1j0I3dJ0uEs7pLUQRb3Rajf5wyuqpE016Kfc++62eWRh/JzBklHM5LinuQK4JvAEuCfqur2UdxnErg8UtKxGPq0TJIlwLeBK4HzgRuSnD/s+2h+Tt9Ik2sUI/eLgJ1V9TOAJPcB1wA/HcG91CzU9M2K6dXs3b3roNjJp57Ogf/95UGx5StXsWfm9eO6l9Q1/b5/RvW9MorivhKY2/sZ4NMjuI/mGHT6pt+bQL/ifLR4v/scGvu32/54oPsM+4W9kN880oe1kL+Xk6oa7hdMrgf+qKr+rJ1/Ebioqv7ikHabgE3t9HeAl4fakRPbMuDNcXdiTMx9Mpn7aPx2VU31uzCKkfsMsGrO+TSw59BGVXUXcNcI7n/CS7KtqtaNux/jYO7mPmnGlfso1rn/BFib5JwkpwJfAB4awX0kSUcw9JF7VR1I8ufAI/SWQt5TVS8N+z6SpCMbyTr3qnoYeHgUX7sjJnI6qjH3yWTuC2zoH6hKksbPZ8tIUgdZ3EcgyT1J9iV5cU7sjCSPJXml7T/e4knyrSQ7kzyf5FPj6/nxSbIqyRNJdiR5KcnNLT4JuZ+e5Kkkz7Xcb2vxc5I82XK/vy0yIMlp7Xxnu75mnP0fhiRLkjyTZEs7n4jck7yW5IUkzybZ1mJjf81b3EfjO8AVh8RuAbZW1VpgazuH3mMa1rZtE3DnAvVxFA4AX6mq84D1wE3t0ROTkPuvgEur6pPABcAVSdYDXwPuaLm/DWxs7TcCb1fVucAdrd1idzOwY875JOX+B1V1wZwlj+N/zVeV2wg2YA3w4pzzl4Hl7Xg58HI7/kfghn7tFvsGPAj84aTlDvwG8DS938x+Ezi5xT8DPNKOHwE+045Pbu0y7r4fR87T9IrYpcAWIBOU+2vAskNiY3/NO3JfOGdX1V6Atj+rxfs9rmHlAvdt6NqP2hcCTzIhubdpiWeBfcBjwKvAO1V1oDWZm9+vc2/X3wXOXNgeD9U3gL8EPmjnZzI5uRfwaJLt7Tfv4QR4zfs89/Hr9xe9F/USpiQfBb4PfLmqfn6UP1reqdyr6n3ggiRLgR8C5/Vr1vadyT3J1cC+qtqe5JLZcJ+mncu9ubiq9iQ5C3gsyX8epe2C5e7IfeG8kWQ5QNvva/GBHtewWCQ5hV5h/25V/aCFJyL3WVX1DvAjep87LE0yO4iam9+vc2/XPwa8tbA9HZqLgc8leQ24j97UzDeYjNypqj1tv4/em/pFnACveYv7wnkI2NCON9Cbj56N39g+RV8PvDv749xik94Q/W5gR1V9fc6lSch9qo3YSfIR4LP0Plx8AriuNTs099n/k+uAx6tNwi42VfXXVTVdVWvoPW7k8ar6EyYg9yS/meS3Zo+By4EXORFe8+P+MKKLG/A9YC/wHr136o305hS3Aq+0/Rmtbej9cZNXgReAdePu/3Hk/fv0fsR8Hni2bVdNSO6/BzzTcn8RuLXFPwE8BewE/gU4rcVPb+c72/VPjDuHIf0/XAJsmZTcW47Pte0l4G9bfOyveX9DVZI6yGkZSeogi7skdZDFXZI6yOIuSR1kcZekDrK4S1IHWdwlqYMs7pLUQf8H12Dhh/D+aYIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "len_list_for_train=[]\n",
    "len_list_for_test=[]\n",
    "for id in train_tokens_ids:\n",
    "    len_list_for_train.append(len(id))\n",
    "for id in test_tokens_ids:\n",
    "    len_list_for_test.append(len(id))\n",
    "#通过绘制直方图可以看出，绝大部分句子tokenize之后的长度都在200左右，故统一长度都为220\n",
    "figure = plt.figure('直方图均衡化',figsize=(10,8))         \n",
    "plt.subplot(2,1,1) \n",
    "plt.hist(x = len_list_for_train, # 指定绘图数据\n",
    "          bins = 60, # 指定直方图中条块的个数\n",
    "          color = 'steelblue', # 指定直方图的填充色\n",
    "          edgecolor = 'black' # 指定直方图的边框色\n",
    "          )\n",
    "plt.show()\n",
    "plt.subplot(2,1,2) \n",
    "plt.hist(x = len_list_for_test, # 指定绘图数据\n",
    "          bins = 60, # 指定直方图中条块的个数\n",
    "          color = 'steelblue', # 指定直方图的填充色\n",
    "          edgecolor = 'black' # 指定直方图的边框色\n",
    "          )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据进行填充和截断，可以使用keras里的pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据进行填充或截断，使输入都有一样的长度220\n",
    "for id in train_tokens_ids:\n",
    "    while(len(id)>220):\n",
    "        id.pop()\n",
    "    while(len(id)<220):\n",
    "        id.append(0)\n",
    "\n",
    "for id in test_tokens_ids:\n",
    "    while(len(id)>220):\n",
    "        id.pop()\n",
    "    while(len(id)<220):\n",
    "        id.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=list(test_data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试将模型放在GPU上进行训练，但是GPU显存不足，导致无法继续进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3063.243776M'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看分配了多少GPU的空间\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将模型放在GPU上\n",
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf = bert_clf.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'878.137856M'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4  #批处理量大小\n",
    "EPOCHS = 10    #迭代轮数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_label_tensor = torch.tensor(np.array(train_labels).reshape(-1, 1)).float()\n",
    "\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_label_tensor = torch.tensor(np.array(test_labels).reshape(-1, 1)).float()\n",
    "\n",
    "train_dataset = TensorDataset(train_tokens_tensor, train_label_tensor)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(test_tokens_tensor, test_label_tensor)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.86 GiB already allocated; 19.16 MiB free; 3.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-6b60241e49b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprobas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-fa3109985176>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tokens, masks)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mdropout_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlinear_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m         )\n\u001b[0;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m                 )\n\u001b[0;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    369\u001b[0m     ):\n\u001b[0;32m    370\u001b[0m         self_attention_outputs = self.attention(\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         )\n\u001b[0;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    313\u001b[0m     ):\n\u001b[0;32m    314\u001b[0m         self_outputs = self.self(\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m         )\n\u001b[0;32m    317\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.86 GiB already allocated; 19.16 MiB free; 3.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)\n",
    "bert_clf.train()\n",
    "for epoch_num in range(EPOCHS):\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, labels = tuple(t.to(device) for t in batch_data)\n",
    "        probas = bert_clf(token_ids)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
