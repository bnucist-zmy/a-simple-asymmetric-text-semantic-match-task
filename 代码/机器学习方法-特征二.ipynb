{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习方法\n",
    "## 特征二：共享关键词指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据和之前计算的出的文本相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "train_data=pd.read_csv('train.csv')\n",
    "test_data=pd.read_csv('test.csv')\n",
    "train_similarity=pd.read_csv('train_set_similarity.csv')\n",
    "test_similarity=pd.read_csv('test_set_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sim_list=[]\n",
    "for i,row in train_similarity.iterrows():\n",
    "    sim_str=row['similarity']\n",
    "    if(sim_str=='0'):\n",
    "        sim_float=0.0\n",
    "    else:\n",
    "        sim_float=float(sim_str[2:-2])\n",
    "    train_sim_list.append(sim_float)\n",
    "\n",
    "test_sim_list=[]\n",
    "for i,row in test_similarity.iterrows():\n",
    "    sim_str=row['similarity']\n",
    "    if(sim_str=='0'):\n",
    "        sim_float=0.0\n",
    "    else:\n",
    "        sim_float=float(sim_str[2:-2])\n",
    "    test_sim_list.append(sim_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在对分词结果进行清洗的时候，利用wordnet进行词干还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#传入的参数sentence是句子经过tokenize后的token组成的列表\n",
    "wtlem = WordNetLemmatizer()\n",
    "def clean_sentence(sentence):\n",
    "    sentence = filter(lambda x: len(x) > 1, sentence)\n",
    "    word_list=[]\n",
    "    for word in sentence:\n",
    "        word=word.lower()\n",
    "        word=wtlem.lemmatize(word,'v')   #动词词干还原\n",
    "        if word not in stop_words:    #过滤没有意义停用词\n",
    "                if word != '\\t':\n",
    "                    word_list.append(word)\n",
    "    result=\" \".join(word_list)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行关键词提取，question中提取5个关键词，sentence中提取30个关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理训练集\n",
    "train_question_key_word_list=[]\n",
    "train_sentence_key_word_list=[]\n",
    "\n",
    "train_question_len_list=[]\n",
    "train_sentence_len_list=[]\n",
    "\n",
    "KEY_WORD_NUM_QUESTION=5\n",
    "KEY_WORD_NUM_SENTENCE=30\n",
    "for i,row in train_data.iterrows():\n",
    "    question_tokenized=word_tokenize(row['question'])\n",
    "    sentence_tokenized=word_tokenize(row['sentence'])\n",
    "    \n",
    "    train_question_len_list.append(len(question_tokenized))\n",
    "    train_sentence_len_list.append(len(sentence_tokenized))\n",
    "    \n",
    "    question_cleaned=clean_sentence(question_tokenized)\n",
    "    sentence_cleaned=clean_sentence(sentence_tokenized)\n",
    "\n",
    "    \n",
    "    question_key_word=jieba.analyse.extract_tags(question_cleaned,topK=KEY_WORD_NUM_QUESTION)\n",
    "    sentence_key_word=jieba.analyse.extract_tags(sentence_cleaned,topK=KEY_WORD_NUM_SENTENCE)\n",
    "    \n",
    "    train_question_key_word_list.append(question_key_word)\n",
    "    train_sentence_key_word_list.append(sentence_key_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理测试集\n",
    "test_question_key_word_list=[]\n",
    "test_sentence_key_word_list=[]\n",
    "\n",
    "for i,row in test_data.iterrows():\n",
    "    question_tokenized=word_tokenize(row['question'])\n",
    "    sentence_tokenized=word_tokenize(row['sentence'])\n",
    "    \n",
    "    question_cleaned=clean_sentence(question_tokenized)\n",
    "    sentence_cleaned=clean_sentence(sentence_tokenized)\n",
    "\n",
    "    \n",
    "    question_key_word=jieba.analyse.extract_tags(question_cleaned,topK=KEY_WORD_NUM_QUESTION)\n",
    "    sentence_key_word=jieba.analyse.extract_tags(sentence_cleaned,topK=KEY_WORD_NUM_SENTENCE)\n",
    "    \n",
    "    test_question_key_word_list.append(question_key_word)\n",
    "    test_sentence_key_word_list.append(sentence_key_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=list(train_data.label)\n",
    "test_labels=list(test_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "label:0\n",
      "smaple 0 question key word:\n",
      "['third', 'digimon', 'series']\n",
      "smaple 0 sentence key word:\n",
      "['seasons', 'digimon', 'unlike', 'two', 'followed', 'tamers', 'takes', 'darker', 'realistic', 'approach', 'story', 'featuring', 'reincarnate', 'deaths', 'complex', 'character', 'development', 'original', 'japanese']\n",
      "--------\n",
      "label:0\n",
      "smaple 1 question key word:\n",
      "['famous', 'palace', 'located']\n",
      "smaple 1 sentence key word:\n",
      "['london', 'westminster', 'greenwich', 'contains', 'four', 'world', 'heritage', 'sites', 'tower', 'kew', 'gardens', 'site', 'comprising', 'palace', 'abbey', 'st', 'margaret', 'church', 'historic', 'settlement']\n",
      "--------\n",
      "label:0\n",
      "smaple 2 question key word:\n",
      "['starred', 'true', 'love']\n",
      "smaple 2 sentence key word:\n",
      "['show', 'starred', 'ted', 'danson', 'dr', 'john', 'becker', 'doctor', 'operated', 'small', 'practice', 'constantly', 'annoyed', 'patients', 'co', 'workers', 'friends', 'practically', 'everything', 'everybody']\n",
      "--------\n",
      "label:0\n",
      "smaple 3 question key word:\n",
      "['open', 'education', 'sources']\n",
      "smaple 3 sentence key word:\n",
      "['open', 'conventional', 'universities', 'merit', 'system', 'degree', 'currently', 'common', 'education', 'campus', 'although', 'already', 'offer', 'degrees', 'university', 'united', 'kingdom']\n",
      "--------\n",
      "label:1\n",
      "smaple 4 question key word:\n",
      "['nirvana', 'achieved']\n",
      "smaple 4 sentence key word:\n",
      "['theravada', 'buddhism', 'ultimate', 'goal', 'attainment', 'sublime', 'state', 'nirvana', 'achieved', 'practicing', 'noble', 'eightfold', 'path', 'also', 'known', 'middle', 'way', 'thus', 'escaping', 'seen']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"-\"*8)\n",
    "    print(\"label:{}\".format(train_labels[i]))\n",
    "    print(\"smaple {} question key word:\".format(i))\n",
    "    print(train_question_key_word_list[i])\n",
    "    print(\"smaple {} sentence key word:\".format(i))\n",
    "    print(train_sentence_key_word_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算“共享关键词指数”，利用wordnet考虑近义词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_common_index=[]\n",
    "for i in range(len(train_question_key_word_list)):\n",
    "    question_key_words=train_question_key_word_list[i]\n",
    "    sentence_key_words=train_sentence_key_word_list[i]\n",
    "    common_list=[word for word in question_key_words if word in sentence_key_words]\n",
    "    \n",
    "    #考虑有没有近义词\n",
    "    if(len(common_list)==0):\n",
    "        for word in question_key_words:\n",
    "            for ss in wn.synsets(word):\n",
    "                for sentence_word in sentence_key_words:\n",
    "                    if sentence_word in ss.lemma_names():\n",
    "                        common_list.append(word)\n",
    "        if(len(common_list)>KEY_WORD_NUM_QUESTION):\n",
    "            common_list=common_list[:KEY_WORD_NUM_QUESTION]\n",
    "    \n",
    "    common_index=float(len(common_list)/KEY_WORD_NUM_QUESTION)\n",
    "    train_common_index.append(common_index)\n",
    "    \n",
    "test_common_index=[]\n",
    "for i in range(len(test_question_key_word_list)):\n",
    "    question_key_words=test_question_key_word_list[i]\n",
    "    sentence_key_words=test_sentence_key_word_list[i]\n",
    "    common_list=[word for word in question_key_words if word in sentence_key_words]\n",
    "    \n",
    "    #考虑有没有近义词\n",
    "    if(len(common_list)==0):\n",
    "        for word in question_key_words:\n",
    "            for ss in wn.synsets(word):\n",
    "                for sentence_word in sentence_key_words:\n",
    "                    if sentence_word in ss.lemma_names():\n",
    "                        common_list.append(word)\n",
    "        if(len(common_list)>KEY_WORD_NUM_QUESTION):\n",
    "            common_list=common_list[:KEY_WORD_NUM_QUESTION]\n",
    "            \n",
    "    common_index=float(len(common_list)/KEY_WORD_NUM_QUESTION)\n",
    "    test_common_index.append(common_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_common_index))\n",
    "print(len(train_sim_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dic={'common_index':train_common_index,'similarity':train_sim_list}\n",
    "test_dic={'common_index':test_common_index,'similarity':test_sim_list}\n",
    "X=pd.DataFrame(train_dic)\n",
    "y_label=np.array(train_labels)\n",
    "X_test=pd.DataFrame(test_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier  #决策树\n",
    "from sklearn.linear_model import LogisticRegression #logistic回归\n",
    "from sklearn.svm import SVC, LinearSVC   #SVC\n",
    "from sklearn.ensemble import RandomForestClassifier  #随机森林\n",
    "from sklearn.naive_bayes import GaussianNB   #朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练并输出预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression准确率：0.74625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "##训练\n",
    "LR.fit(X,y_label)\n",
    "##预测\n",
    "LR_prediction = LR.predict(X_test)\n",
    "output1 = pd.DataFrame({'label': LR_prediction})\n",
    "\n",
    "pred=np.array(output1.label)\n",
    "ground_truth=np.array(test_labels)\n",
    "res = (ground_truth == pred)\n",
    "acc = res.sum()/len(res)\n",
    "print(\"logistic regression准确率：{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树准确率：0.654375\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X, y_label)\n",
    "DT_prediction = decision_tree.predict(X_test)\n",
    "output2 = pd.DataFrame({ 'label': DT_prediction})\n",
    "\n",
    "pred=np.array(output2.label)\n",
    "ground_truth=np.array(test_labels)\n",
    "res = (ground_truth == pred)\n",
    "acc = res.sum()/len(res)\n",
    "print(\"决策树准确率：{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC准确率：0.75\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X, y_label)\n",
    "SVC_prediction = svc.predict(X_test)\n",
    "output3 = pd.DataFrame({'label':SVC_prediction})\n",
    "\n",
    "pred=np.array(output3.label)\n",
    "ground_truth=np.array(test_labels)\n",
    "res = (ground_truth == pred)\n",
    "acc = res.sum()/len(res)\n",
    "print(\"SVC准确率：{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC准确率：0.738125\n"
     ]
    }
   ],
   "source": [
    "RFCmodel = RandomForestClassifier(n_estimators=20, max_depth=2, random_state=1)\n",
    "RFCmodel.fit(X, y_label)\n",
    "predictions = RFCmodel.predict(X_test)\n",
    "output4 = pd.DataFrame({'label': predictions})\n",
    "\n",
    "pred=np.array(output4.label)\n",
    "ground_truth=np.array(test_labels)\n",
    "res = (ground_truth == pred)\n",
    "acc = res.sum()/len(res)\n",
    "print(\"RFC准确率：{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朴素贝叶斯准确率：0.733125\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X, y_label)\n",
    "predictions = clf.predict(X_test)\n",
    "output5 = pd.DataFrame({'label': predictions})\n",
    "\n",
    "pred=np.array(output5.label)\n",
    "ground_truth=np.array(test_labels)\n",
    "res = (ground_truth == pred)\n",
    "acc = res.sum()/len(res)\n",
    "print(\"朴素贝叶斯准确率：{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出最好的结果到prediction.csv中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile=open('prediction.csv','w',encoding='utf-8')\n",
    "for i,row in output3.iterrows():\n",
    "    print(row['label'],file=myfile)\n",
    "#output4.to_csv('C:/Users/xx/Desktop/SVC_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
